{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4fecbd3e",
   "metadata": {},
   "source": [
    "## Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6ae66492",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from gymnasium.wrappers import TimeLimit\n",
    "\n",
    "from common import *\n",
    "from hyperparameters import *\n",
    "from plotting import *\n",
    "\n",
    "from dp import value_iteration, compute_policy\n",
    "from mpc import CartpoleMPC\n",
    "from q_learning import run_q_learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa581a50",
   "metadata": {},
   "source": [
    "## Evaluator Function (as a Helper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7e03b07e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_method(name, returns):\n",
    "    print(f\"\\n===== {name} =====\")\n",
    "    print(f\"Mean       : {np.mean(returns):.4f}\")\n",
    "    print(f\"Std        : {np.std(returns):.4f}\")\n",
    "    print(f\"Min        : {np.min(returns):.4f}\")\n",
    "    print(f\"Max        : {np.max(returns):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0bcfafd",
   "metadata": {},
   "source": [
    "## 1. DP Simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5de48a94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Value Iteration...\n"
     ]
    },
    {
     "ename": "NotImplementedError",
     "evalue": "Quadratic cost function not implemented",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNotImplementedError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      2\u001b[39m env = TimeLimit(env, max_episode_steps=MAX_EPISODE_STEPS)\n\u001b[32m      4\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mRunning Value Iteration...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m V = \u001b[43mvalue_iteration\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      6\u001b[39m policy_dp = compute_policy(V)\n\u001b[32m      8\u001b[39m returns_dp = evaluate_agent(env, \u001b[38;5;28mtype\u001b[39m=\u001b[33m\"\u001b[39m\u001b[33mDP\u001b[39m\u001b[33m\"\u001b[39m, policy=policy_dp)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\acer\\OneDrive\\Desktop\\UofE - Semester 2\\Robot and Reinforcement Learning\\Project 1\\rrl_cartpole_control-coursework-1-\\dp.py:13\u001b[39m, in \u001b[36mvalue_iteration\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mglobal\u001b[39;00m V\n\u001b[32m     12\u001b[39m \u001b[38;5;66;03m# Pre-calculate the current state costs for efficiency\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m current_state_costs = \u001b[43mquadratic_cost\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate_tensor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m it \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(N_ITERATIONS):\n\u001b[32m     16\u001b[39m     V_old = V.copy()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\acer\\OneDrive\\Desktop\\UofE - Semester 2\\Robot and Reinforcement Learning\\Project 1\\rrl_cartpole_control-coursework-1-\\common.py:74\u001b[39m, in \u001b[36mquadratic_cost\u001b[39m\u001b[34m(state)\u001b[39m\n\u001b[32m     72\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mquadratic_cost\u001b[39m(state):\n\u001b[32m     73\u001b[39m     \u001b[38;5;66;03m### FILL IN HERE ### hint: CART_COST_WEIGHT, POLE_ANGLE_COST_WEIGHT\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m74\u001b[39m     x, x_dot, theta, theta_dot = state\n\u001b[32m     75\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m CART_COST_WEIGHT * x**\u001b[32m2\u001b[39m + POLE_ANGLE_COST_WEIGHT * theta**\u001b[32m2\u001b[39m + TERMINAL_COST\n",
      "\u001b[31mNotImplementedError\u001b[39m: Quadratic cost function not implemented"
     ]
    }
   ],
   "source": [
    "env = CartPoleEnv(x_threshold=X_LIMIT, theta_threshold_radians=THETA_LIMIT)\n",
    "env = TimeLimit(env, max_episode_steps=MAX_EPISODE_STEPS)\n",
    "\n",
    "print(\"Running Value Iteration...\")\n",
    "V = value_iteration()\n",
    "policy_dp = compute_policy(V)\n",
    "\n",
    "returns_dp = evaluate_agent(env, type=\"DP\", policy=policy_dp)\n",
    "evaluate_method(\"DP\", returns_dp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f21a819",
   "metadata": {},
   "source": [
    "## 2. MPC Simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8b5d5939",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running MPC...\n"
     ]
    },
    {
     "ename": "NotImplementedError",
     "evalue": "Quadratic cost function not implemented",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNotImplementedError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      5\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mRunning MPC...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      6\u001b[39m mpc = CartpoleMPC(H=\u001b[32m5\u001b[39m, max_iters=\u001b[32m5\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m returns_mpc = \u001b[43mevaluate_agent\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mtype\u001b[39;49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mMPC\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpolicy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmpc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      9\u001b[39m evaluate_method(\u001b[33m\"\u001b[39m\u001b[33mMPC\u001b[39m\u001b[33m\"\u001b[39m, returns_mpc)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\acer\\OneDrive\\Desktop\\UofE - Semester 2\\Robot and Reinforcement Learning\\Project 1\\rrl_cartpole_control-coursework-1-\\common.py:108\u001b[39m, in \u001b[36mevaluate_agent\u001b[39m\u001b[34m(env, type, policy, Q)\u001b[39m\n\u001b[32m    106\u001b[39m     action = np.argmax(Q[indices])\n\u001b[32m    107\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33mDP\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m108\u001b[39m     action = policy[indices]\n\u001b[32m    109\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33mMPC\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    110\u001b[39m     action = policy.control(state)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\acer\\OneDrive\\Desktop\\UofE - Semester 2\\Robot and Reinforcement Learning\\Project 1\\rrl_cartpole_control-coursework-1-\\common.py:69\u001b[39m, in \u001b[36mreward_function\u001b[39m\u001b[34m(state, terminated)\u001b[39m\n\u001b[32m     67\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m terminated:\n\u001b[32m     68\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m TERMINAL_COST\n\u001b[32m---> \u001b[39m\u001b[32m69\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mquadratic_cost\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\acer\\OneDrive\\Desktop\\UofE - Semester 2\\Robot and Reinforcement Learning\\Project 1\\rrl_cartpole_control-coursework-1-\\common.py:74\u001b[39m, in \u001b[36mquadratic_cost\u001b[39m\u001b[34m(state)\u001b[39m\n\u001b[32m     72\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mquadratic_cost\u001b[39m(state):\n\u001b[32m     73\u001b[39m     \u001b[38;5;66;03m### FILL IN HERE ### hint: CART_COST_WEIGHT, POLE_ANGLE_COST_WEIGHT\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m74\u001b[39m     x, x_dot, theta, theta_dot = state\n\u001b[32m     75\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m CART_COST_WEIGHT * x**\u001b[32m2\u001b[39m + POLE_ANGLE_COST_WEIGHT * theta**\u001b[32m2\u001b[39m + TERMINAL_COST\n",
      "\u001b[31mNotImplementedError\u001b[39m: Quadratic cost function not implemented"
     ]
    }
   ],
   "source": [
    "env = CartPoleEnv(x_threshold=X_LIMIT, theta_threshold_radians=THETA_LIMIT, continuous_action=True)\n",
    "\n",
    "env = TimeLimit(env, max_episode_steps=MAX_EPISODE_STEPS)\n",
    "\n",
    "print(\"Running MPC...\")\n",
    "mpc = CartpoleMPC(H=5, max_iters=5)\n",
    "\n",
    "returns_mpc = evaluate_agent(env, type=\"MPC\", policy=mpc)\n",
    "evaluate_method(\"MPC\", returns_mpc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5b3ad34",
   "metadata": {},
   "source": [
    "## 3. Q-Learning Simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6471b9e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Q-Learning...\n"
     ]
    },
    {
     "ename": "NotImplementedError",
     "evalue": "Quadratic cost function not implemented",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNotImplementedError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      2\u001b[39m train_env = TimeLimit(train_env, max_episode_steps=MAX_EPISODE_STEPS)\n\u001b[32m      4\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mTraining Q-Learning...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m Q_learning = \u001b[43mrun_q_learning\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_env\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mTRAIN_TIMESTEPS_M\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      7\u001b[39m returns_q = evaluate_agent(train_env, \u001b[38;5;28mtype\u001b[39m=\u001b[33m\"\u001b[39m\u001b[33mQ\u001b[39m\u001b[33m\"\u001b[39m, Q = Q_learning)\n\u001b[32m      8\u001b[39m evaluate_method(\u001b[33m\"\u001b[39m\u001b[33mQ-Learning\u001b[39m\u001b[33m\"\u001b[39m, returns_q)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\acer\\OneDrive\\Desktop\\UofE - Semester 2\\Robot and Reinforcement Learning\\Project 1\\rrl_cartpole_control-coursework-1-\\q_learning.py:60\u001b[39m, in \u001b[36mrun_q_learning\u001b[39m\u001b[34m(env, train_timesteps_M)\u001b[39m\n\u001b[32m     58\u001b[39m next_state, _, terminated, truncated, _ = env.step(\u001b[38;5;28mint\u001b[39m(action))\n\u001b[32m     59\u001b[39m next_indices = state_to_indices(next_state, [x_vals, x_dot_vals, theta_vals, theta_dot_vals])\n\u001b[32m---> \u001b[39m\u001b[32m60\u001b[39m reward = \u001b[43mreward_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnext_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mterminated\u001b[49m\u001b[43m)\u001b[49m * -\u001b[32m1\u001b[39m\n\u001b[32m     61\u001b[39m episode_reward += reward\n\u001b[32m     63\u001b[39m \u001b[38;5;66;03m# Q-value update\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\acer\\OneDrive\\Desktop\\UofE - Semester 2\\Robot and Reinforcement Learning\\Project 1\\rrl_cartpole_control-coursework-1-\\common.py:69\u001b[39m, in \u001b[36mreward_function\u001b[39m\u001b[34m(state, terminated)\u001b[39m\n\u001b[32m     67\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m terminated:\n\u001b[32m     68\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m TERMINAL_COST\n\u001b[32m---> \u001b[39m\u001b[32m69\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mquadratic_cost\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\acer\\OneDrive\\Desktop\\UofE - Semester 2\\Robot and Reinforcement Learning\\Project 1\\rrl_cartpole_control-coursework-1-\\common.py:74\u001b[39m, in \u001b[36mquadratic_cost\u001b[39m\u001b[34m(state)\u001b[39m\n\u001b[32m     72\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mquadratic_cost\u001b[39m(state):\n\u001b[32m     73\u001b[39m     \u001b[38;5;66;03m### FILL IN HERE ### hint: CART_COST_WEIGHT, POLE_ANGLE_COST_WEIGHT\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m74\u001b[39m     x, x_dot, theta, theta_dot = state\n\u001b[32m     75\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m CART_COST_WEIGHT * x**\u001b[32m2\u001b[39m + POLE_ANGLE_COST_WEIGHT * theta**\u001b[32m2\u001b[39m + TERMINAL_COST\n",
      "\u001b[31mNotImplementedError\u001b[39m: Quadratic cost function not implemented"
     ]
    }
   ],
   "source": [
    "train_env = CartPoleEnv(x_threshold=X_LIMIT, theta_threshold_radians=THETA_LIMIT)\n",
    "train_env = TimeLimit(train_env, max_episode_steps=MAX_EPISODE_STEPS)\n",
    "\n",
    "print(\"Training Q-Learning...\")\n",
    "Q_learning = run_q_learning(train_env, TRAIN_TIMESTEPS_M)\n",
    "\n",
    "returns_q = evaluate_agent(train_env, type=\"Q\", Q = Q_learning)\n",
    "evaluate_method(\"Q-Learning\", returns_q)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fe6f28f",
   "metadata": {},
   "source": [
    "## 4. Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "27101ac1",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'returns_dp' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m plt.figure(figsize=(\u001b[32m7\u001b[39m,\u001b[32m5\u001b[39m))\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m plt.boxplot([\u001b[43mreturns_dp\u001b[49m, returns_mpc, returns_q], labels=[\u001b[33m\"\u001b[39m\u001b[33mDP\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mMPC\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mQ-Learning\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m      5\u001b[39m plt.ylabel(\u001b[33m\"\u001b[39m\u001b[33mReturn\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      6\u001b[39m plt.title(\u001b[33m\"\u001b[39m\u001b[33mPerformance Comparison\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'returns_dp' is not defined"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 700x500 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(7,5))\n",
    "\n",
    "plt.boxplot([returns_dp, returns_mpc, returns_q], labels=[\"DP\", \"MPC\", \"Q-Learning\"])\n",
    "\n",
    "plt.ylabel(\"Return\")\n",
    "plt.title(\"Performance Comparison\")\n",
    "plt.grid(True)\n",
    "plt.legend(True)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
